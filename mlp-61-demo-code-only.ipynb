{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Implementation of Ada-Boost Algorithm","metadata":{}},{"cell_type":"code","source":"#Importing neccesary packages\n# Load libraries\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:45:29.701737Z","iopub.execute_input":"2023-07-12T14:45:29.702149Z","iopub.status.idle":"2023-07-12T14:45:31.775406Z","shell.execute_reply.started":"2023-07-12T14:45:29.702115Z","shell.execute_reply":"2023-07-12T14:45:31.773765Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load data\niris = datasets.load_iris()\nX = iris.data\ny = iris.target","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:45:31.779112Z","iopub.execute_input":"2023-07-12T14:45:31.780007Z","iopub.status.idle":"2023-07-12T14:45:31.796168Z","shell.execute_reply.started":"2023-07-12T14:45:31.779955Z","shell.execute_reply":"2023-07-12T14:45:31.794423Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # 80% training and 20% test","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:45:31.799053Z","iopub.execute_input":"2023-07-12T14:45:31.799561Z","iopub.status.idle":"2023-07-12T14:45:31.814934Z","shell.execute_reply.started":"2023-07-12T14:45:31.799498Z","shell.execute_reply":"2023-07-12T14:45:31.813364Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Create adaboost classifer object\nAdaModel = AdaBoostClassifier(n_estimators=100,\n                         learning_rate=1)\n# Train Adaboost Classifer\nmodel = AdaModel.fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:45:31.818486Z","iopub.execute_input":"2023-07-12T14:45:31.820230Z","iopub.status.idle":"2023-07-12T14:45:32.097239Z","shell.execute_reply.started":"2023-07-12T14:45:31.820048Z","shell.execute_reply":"2023-07-12T14:45:32.095950Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#Important Parameters\nbase_estimator: It is a weak learner used to train the model. It uses DecisionTreeClassifier as default weak learner for training purpose. You can also specify different machine learning algorithms.\n\nn_estimators: Number of weak learners to train iteratively.\n\nlearning_rate: It contributes to the weights of weak learners. It uses 1 as a default value.","metadata":{}},{"cell_type":"code","source":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:45:32.099165Z","iopub.execute_input":"2023-07-12T14:45:32.100909Z","iopub.status.idle":"2023-07-12T14:45:32.109834Z","shell.execute_reply.started":"2023-07-12T14:45:32.100857Z","shell.execute_reply":"2023-07-12T14:45:32.108583Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Accuracy: 0.9\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import Support Vector Classifier\nfrom sklearn.svm import SVC\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\nsvc=SVC(probability=True, kernel='linear')\n\n# Create adaboost classifer object\nabc =AdaBoostClassifier(n_estimators=50, base_estimator=svc,learning_rate=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:45:32.111648Z","iopub.execute_input":"2023-07-12T14:45:32.112019Z","iopub.status.idle":"2023-07-12T14:45:32.123170Z","shell.execute_reply.started":"2023-07-12T14:45:32.111980Z","shell.execute_reply":"2023-07-12T14:45:32.121622Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Train Adaboost Classifer\nmodel = abc.fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:45:32.125318Z","iopub.execute_input":"2023-07-12T14:45:32.126228Z","iopub.status.idle":"2023-07-12T14:45:32.426685Z","shell.execute_reply.started":"2023-07-12T14:45:32.126182Z","shell.execute_reply":"2023-07-12T14:45:32.425509Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-07-12T14:45:32.428070Z","iopub.execute_input":"2023-07-12T14:45:32.428410Z","iopub.status.idle":"2023-07-12T14:45:32.435755Z","shell.execute_reply.started":"2023-07-12T14:45:32.428381Z","shell.execute_reply":"2023-07-12T14:45:32.434376Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Accuracy: 0.9333333333333333\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Pros\nAdaBoost is easy to implement. It iteratively corrects the mistakes of the weak classifier and improves accuracy by combining weak learners. You can use many base classifiers with AdaBoost. AdaBoost is not prone to overfitting. This can be found out via experiment results, but there is no concrete reason available.\n\nCons\nAdaBoost is sensitive to noise data. It is highly affected by outliers because it tries to fit each point perfectly. AdaBoost is slower compared to XGBoost.","metadata":{}}]}